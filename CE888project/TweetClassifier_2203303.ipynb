{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_tweetDetect_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudeep93/CE888/blob/main/CE888project/TweetClassifier_2203303.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZKhkIXyDvrG"
      },
      "source": [
        "## Installing the transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PNIPkMuDxz3"
      },
      "source": [
        "!pip install transformers\n",
        "!pip3 install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCDgO-B1eb_n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7kYO7DwGfPk"
      },
      "source": [
        "### download punkt and stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR16HtTfGeI7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJwuXEnoD7ZR"
      },
      "source": [
        "### Import Required Librearies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9trS7RXDz5Q"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import imblearn\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import emoji\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input\n",
        "from tensorflow.keras import regularizers\n",
        "import pickle\n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, AutoModel, AutoTokenizer,TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "import keras\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm7Q2IfOExik"
      },
      "source": [
        "## Function to Clean the Text tweet Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ff8egzkEvHU"
      },
      "source": [
        "pd.set_option('display.max_colwidth',1000)\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "# clean the text dataframe\n",
        "def text_cleaning(text):\n",
        "  text = text.lower()\n",
        "        \n",
        "  text = re.sub(r'http.?:\\/\\/\\S+','',text) #remove links\n",
        "  text = emoji.demojize(text, delimiters=(\"\", \"\")) \n",
        "  # text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "  text \n",
        "  text=re.sub(r'@[A-Za-z0-9]+','',text) #removes mentions\n",
        "  text=re.sub(r'#','',text)#remove hashtag\n",
        "  text = re.sub(r'RT[\\s]','',text) #remove RT\n",
        "  text = re.sub(r'http?:\\/\\/\\S+','',text) #remove links\n",
        "  text = re.sub(r\"i'm\", \"i am\", text)\n",
        "  text = re.sub(r\"isn’t \", \"is not\", text)\n",
        "  text = re.sub(r\"y'all\", \"you all\", text)\n",
        "  text = re.sub(r\"he's\", \"he is\", text)\n",
        "  text = re.sub(r\"she's\", \"she is\", text)\n",
        "  text = re.sub(r\"here's\", \"here is\", text)\n",
        "  text = re.sub(r\"that's\", \"that is\", text)        \n",
        "  text = re.sub(r\"what's\", \"what is\", text)\n",
        "  text = re.sub(r\"where's\", \"where is\", text) \n",
        "  text = re.sub(r\"\\’s\", \" is\", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will\", text)  \n",
        "  text = re.sub(r\"they'll\", \"they will\", text)  \n",
        "  text = re.sub(r\"\\'ve\", \" have\", text)  \n",
        "  text = re.sub(r\"\\'re\", \" are\", text)\n",
        "  text = re.sub(r\"\\'d\", \" would\", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "  text = re.sub(r\"won't\", \"will not\", text)\n",
        "  text = re.sub(r\"don't\", \"do not\", text)\n",
        "  text = re.sub(r\"did't\", \"did not\", text)\n",
        "  text = re.sub(r\"did’t\", \"did not\", text)\n",
        "  text = re.sub(r\"can't\", \"can not\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "  text = re.sub(r\"have't\", \"have not\", text)\n",
        "  text = re.sub(r\"qt\", \"\", text)\n",
        "  text = re.sub(r\"_\", \" \", text)\n",
        "  text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \" \", text)\n",
        "  text=re.sub(r'\\'','',text)\n",
        "  text=re.sub(r'\\\"','',text)\n",
        "  text=re.sub(r'\\“','',text)\n",
        "  text=re.sub(r'\\”','',text)\n",
        "  tokens = word_tokenize(text)\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in tokens]\n",
        "  words1 = [word for word in stripped if word.isalpha()]\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  stop_words.discard(\"not\")\n",
        "\n",
        "  words1 = [w for w in words1 if not w in stop_words]\n",
        "  words1 = ' '.join(words1)\n",
        "  # print(words1)\n",
        "  # words1 = \" \".join(w for w in nltk.wordpunct_tokenize(words1) if w.lower() in words or not w.isalpha())\n",
        "  # words1 = [w for w in words1 if  w in words.words()]\n",
        "  # final = ' '.join(words1)\n",
        "  final=[lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(words1)]\n",
        "  final = ' '.join(final)\n",
        "  return words1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-c4UzFxB5b0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtw3W3b1FC8U"
      },
      "source": [
        "### Apply cleaning function on Train  and test dataset and importing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq3PoEX7FB5W"
      },
      "source": [
        "def dataset_cleaning(path_tweet,path_label):\n",
        "  # load data\n",
        "  print(\"cleaing tweets on path\",path_tweet)\n",
        "  col_names=['tweets', 'labels'] \n",
        "  df = pd.read_csv(path_tweet, sep =\"\\n\",names=col_names,header=None)\n",
        "    # load data label\n",
        "  labels = pd.read_csv(path_label, sep =\"\\n\",header=None)\n",
        "  df['labels']=labels\n",
        "  # Cleaning data \n",
        "  df['tweets']=  df['tweets'].apply(text_cleaning)\n",
        "\n",
        "  print(\"Finished cleaing tweets on path\",path_tweet,\"\\n \\n\")\n",
        "  return df\n",
        "\n",
        "hate_train=dataset_cleaning('/content/drive/MyDrive/datasets/hate/train_text.txt','/content/drive/MyDrive/datasets/hate/train_labels.txt')\n",
        "hate_test=dataset_cleaning('/content/drive/MyDrive/datasets/hate/test_text.txt','/content/drive/MyDrive/datasets/hate/test_labels.txt')\n",
        "hate_val=dataset_cleaning('/content/drive/MyDrive/datasets/hate/val_text.txt','/content/drive/MyDrive/datasets/hate/val_labels.txt')\n",
        "\n",
        "irony_train=dataset_cleaning('/content/drive/MyDrive/datasets/irony/train_text.txt','/content/drive/MyDrive/datasets/irony/train_labels.txt')\n",
        "irony_test=dataset_cleaning('/content/drive/MyDrive/datasets/irony/test_text.txt','/content/drive/MyDrive/datasets/irony/test_labels.txt')\n",
        "irony_val=dataset_cleaning('/content/drive/MyDrive/datasets/irony/val_text.txt','/content/drive/MyDrive/datasets/irony/val_labels.txt')\n",
        "\n",
        "\n",
        "offensive_train=dataset_cleaning('/content/drive/MyDrive/datasets/offensive/train_text.txt','/content/drive/MyDrive/datasets/offensive/train_labels.txt')\n",
        "offensive_test=dataset_cleaning('/content/drive/MyDrive/datasets/offensive/test_text.txt','/content/drive/MyDrive/datasets/offensive/test_labels.txt')\n",
        "offensive_val=dataset_cleaning('/content/drive/MyDrive/datasets/offensive/val_text.txt','/content/drive/MyDrive/datasets/offensive/val_labels.txt')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnQkqOzZkk5t"
      },
      "source": [
        "irony_train['tweets'].tail(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd76foEzbZ-F"
      },
      "source": [
        "##Label Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If85Fzs_bh73"
      },
      "source": [
        "### Hate label distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_QsLuvSUgMJ"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"labels\", data=hate_train).set_title('Hate Labels')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynlQS0rwbqOq"
      },
      "source": [
        "### Irony label distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UazsfA9gbqOr"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"labels\", data=irony_train).set_title('Irony Labels')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU_F4k3WbpD5"
      },
      "source": [
        "### Offensive label distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbIz_q8ibpD6"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"labels\", data=offensive_train).set_title('Offensive Labels')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htq9T3ALG6ZQ"
      },
      "source": [
        "### Frequency of the words visaulization test set in Hate,Irony and Offensive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzCZm_csG5Ve"
      },
      "source": [
        "majorwords=' '.join([twt for twt in hate_train['tweets']])\n",
        "majorwords=WordCloud(width=1000,height=500,random_state=32,max_font_size=130).generate(majorwords)\n",
        "# plt.imshow(majorwords,interpolation='bilinear')\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(majorwords)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwoQOom68Iqg"
      },
      "source": [
        "majorwords=' '.join([twt for twt in irony_train['tweets']])\n",
        "majorwords=WordCloud(width=1000,height=500,random_state=32,max_font_size=130).generate(majorwords)\n",
        "# plt.imshow(majorwords,interpolation='bilinear')\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(majorwords)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwbkjn2Q8LX7"
      },
      "source": [
        "majorwords=' '.join([twt for twt in offensive_train['tweets']])\n",
        "majorwords=WordCloud(width=1000,height=500,random_state=32,max_font_size=130).generate(majorwords)\n",
        "# plt.imshow(majorwords,interpolation='bilinear')\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(majorwords)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pTJhzkp2Gui"
      },
      "source": [
        "irony_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCILgTh3plcO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TZq7Vgd0j5X"
      },
      "source": [
        "##Loading the Pre-trained DistilBERT model\n",
        "Let's now load a pre-trained DistilBERT model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz5sYnvZ-niO"
      },
      "source": [
        "\n",
        "# Loading DistilBERT Tokenizer and the DistilBERT model\n",
        "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76rquoDNNHH1"
      },
      "source": [
        "## Check the MAX_length of Entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93kzzNv3_bAg"
      },
      "source": [
        "\n",
        "\n",
        "def max_length(df_train,df_val,df_test):\n",
        "  b=[]\n",
        "  test=pd.concat([df_train['tweets'], df_val['tweets'],df_test['tweets']], ignore_index=True, sort=True)\n",
        "  for i in test:\n",
        "    a=dbert_tokenizer.tokenize(i)\n",
        "    b.append(len(a))\n",
        "  df_t = pd.DataFrame(b, columns=['A'])\n",
        "  return df_t.nlargest(5,'A', keep='all')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve24j8BGNOYq"
      },
      "source": [
        "## MAX LENGHT OF HATE IRONY AND OFFENSIVE FOR PADDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSmJCkPtJv3-"
      },
      "source": [
        "max_len_hate=max_length(hate_test,hate_train,hate_val)\n",
        "max_len_irony=max_length(irony_test,irony_train,irony_val)\n",
        "max_len_offensive=max_length(offensive_test,offensive_train,offensive_val)\n",
        "print(\"FOR HATE\",max_len_hate,\"\\n\")\n",
        "print(\"FOR IRONY\",max_len_irony,\"\\n\")\n",
        "print(\"FOR OFFENSIVE\",max_len_offensive,\"\\n\")\n",
        "\n",
        "max_len=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1EB-bTX0SEd"
      },
      "source": [
        "Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n",
        "\n",
        "## Model #1: Preparing the Dataset\n",
        "Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n",
        "\n",
        "### Tokenization\n",
        "Our first step is to tokenize the sentences -- break them up into word and subwords in the format DistilBERT is comfortable with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbNuNDaDBfx0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PvsMmlMQoMj"
      },
      "source": [
        "## Create NN Model for classification using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXYGIMqWBlTi"
      },
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout= Dropout(0.5)(dense)\n",
        "    pred = Dense(2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz58cISzQzpA"
      },
      "source": [
        "## Create Three models for Three datasets ie hate irony and offensive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2z36kFeoM40"
      },
      "source": [
        "model_hate=create_model()\n",
        "model_irony=create_model()\n",
        "model_offensive=create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQ7DCQU9tLw"
      },
      "source": [
        "##FUCNTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMdYgyT-9rLW"
      },
      "source": [
        "def distil_bert_process(df,max_len):\n",
        "# Preparing input for the model\n",
        "\n",
        "  sentences=df['tweets']\n",
        "  dbert_inp=dbert_tokenizer.encode_plus(sentences[0],add_special_tokens = True,max_length =max_len,pad_to_max_length = True,truncation=True)\n",
        "  dbert_inp\n",
        "  print(\"Tokenization In-process\")\n",
        "  id_inp=np.asarray(dbert_inp['input_ids'])\n",
        "  mask_inp=np.asarray(dbert_inp['attention_mask'])\n",
        "  out=dbert_model([id_inp.reshape(1,-1),mask_inp.reshape(1,-1)])\n",
        "  type(out),out\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for sent in sentences:\n",
        "      dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "      input_ids.append(dbert_inps['input_ids'])\n",
        "      attention_masks.append(dbert_inps['attention_mask'])\n",
        "\n",
        "  input_ids=np.array(input_ids) \n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(df['labels'])\n",
        "  print(\"Tokenization finished\")\n",
        "  return input_ids,attention_masks,labels\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-mBS08LjUBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0hzYWgBTaQD"
      },
      "source": [
        "def training(df_train,df_val,model,no_epochs,max_len,save_path):\n",
        "# train dataset dbert process\n",
        "  id_train,att_train,label_train=distil_bert_process(df_train,max_len)\n",
        "\n",
        "# val dataset dbert process \n",
        "  id_val,att_val,label_val=distil_bert_process(df_val,max_len)\n",
        "\n",
        "  print(\"Model Builing in process\")\n",
        "\n",
        "  log_dir='dbert_model'\n",
        "  model_save_path='./{}.h5'.format(save_path)\n",
        "\n",
        "  callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "  print(\"Compiling Model\")\n",
        "  model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
        "  callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "  print(\"Fitting Model\")\n",
        "  history=model.fit([id_train,att_train],label_train,batch_size=16,epochs=no_epochs,validation_data=([id_val,att_val],label_val),callbacks=callbacks)\n",
        "  print(\"Fitting finished\")\n",
        "  return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwFyu28sWX6w"
      },
      "source": [
        "###  Training and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUVB1-DwYYe2"
      },
      "source": [
        "#### Hate Dataset training and predicting and evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYc83GinWWv5"
      },
      "source": [
        "# Train data\n",
        "training_hate=training(hate_train,hate_val,model_hate,8,100,\"hate_model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEDHNWB9dQDT"
      },
      "source": [
        "# Plot results\n",
        "acc = training_hate.history['accuracy']\n",
        "val_acc = training_hate.history['val_accuracy']\n",
        "loss = training_hate.history['loss']\n",
        "val_loss = training_hate.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoQUnpcNUXmj"
      },
      "source": [
        "id_hate_test,att_hate_test,label_hate_test=distil_bert_process(hate_test,100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Zj6dCIUXmr"
      },
      "source": [
        "pred_labels_hate=model_hate.predict([id_hate_test,att_hate_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7851d8PmAyv"
      },
      "source": [
        "y_pred_bool_hate = np.argmax(pred_labels_hate, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-kIGZ80O2Fn"
      },
      "source": [
        "# Classification Report for HATE DATASET\n",
        "print('Classification Report for HATE DATASET')\n",
        "print(classification_report(label_hate_test,y_pred_bool_hate))\n",
        "print(accuracy_score(label_hate_test,y_pred_bool_hate))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNObI2RLfIDw"
      },
      "source": [
        "#### Irony Dataset training and predicting and evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xFjUIVgfID6"
      },
      "source": [
        "# Train data\n",
        "training_irony=training(irony_train,irony_val,model_irony,20,100,\"irony_model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnE0UE0TfID8"
      },
      "source": [
        "# Plot results\n",
        "acc = training_irony.history['accuracy']\n",
        "val_acc = training_irony.history['val_accuracy']\n",
        "loss = training_irony.history['loss']\n",
        "val_loss = training_irony.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQTdwPaLfID9"
      },
      "source": [
        "id_irony_test,att_irony_test,label_irony_test=distil_bert_process(irony_test,100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BtVn_nUfID9"
      },
      "source": [
        "pred_labels_irony=model_irony.predict([id_irony_test,att_irony_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_B2MvopfID-"
      },
      "source": [
        "y_pred_bool_irony = np.argmax(pred_labels_irony, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpWVhjB7fID-"
      },
      "source": [
        "# Classification Report for IRONY Dataset\n",
        "print('Classification Report for IRONY Dataset')\n",
        "print(classification_report(label_irony_test,y_pred_bool_irony))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Dqnl4XfCji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWpBoKCDjRs_"
      },
      "source": [
        "#### Offensive Dataset training and predicting and evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5cfiZ_7jRtK"
      },
      "source": [
        "# Train data\n",
        "training_offensive=training(offensive_train,offensive_val,model_offensive,6,100,\"offensive_model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C1mkpdIjRtL"
      },
      "source": [
        "# Plot results\n",
        "acc = training_offensive.history['accuracy']\n",
        "val_acc = training_offensive.history['val_accuracy']\n",
        "loss = training_offensive.history['loss']\n",
        "val_loss = training_offensive.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp0hEOiIjRtM"
      },
      "source": [
        "id_offensive_test,att_offensive_test,label_offensive_test=distil_bert_process(offensive_test,100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOg3o1TpjRtM"
      },
      "source": [
        "pred_labels_offensive=model_offensive.predict([id_offensive_test,att_offensive_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjE2gGUwjRtM"
      },
      "source": [
        "y_pred_bool_offensive = np.argmax(pred_labels_offensive, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCKk6jtMjRtN"
      },
      "source": [
        "# Classification Report for OFFENSIVE Dataset\n",
        "print('Classification Report for OFFENSIVE Dataset')\n",
        "print(classification_report(label_offensive_test,y_pred_bool_offensive))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SstiySXxj_an"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}